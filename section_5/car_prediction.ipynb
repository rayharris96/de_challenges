{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split,RepeatedStratifiedKFold,cross_val_score,GridSearchCV\n",
    "from numpy import mean, std\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from xgboost import XGBClassifier,XGBRegressor,XGBRFClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data into pandas DataFrame\n",
    "df = pd.read_csv('training_data/car.data', names=['buying', 'maint', 'doors', 'persons', 'boot', 'safety', 'class'])\n",
    "#Leave out persons column|\n",
    "df=df[['maint','doors','boot','safety','class','buying']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vhigh    0.25\n",
       "high     0.25\n",
       "med      0.25\n",
       "low      0.25\n",
       "Name: buying, dtype: float64"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the distribution of target column\n",
    "df['buying'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target column distribution is quite balanced\n",
    "\n",
    "#### Changing categorical range to numerical range (quantifying good, very good etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['vhigh', 'high', 'med', 'low'], dtype=object)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking unique values\n",
    "df['maint'].unique()\n",
    "df['doors'].unique()\n",
    "df['safety'].unique()\n",
    "df['class'].unique()\n",
    "df['buying'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantify(data):\n",
    "    if data in [\"low\",\"unacc\",\"small\"]:\n",
    "        return 0\n",
    "    elif data in [\"med\",\"acc\"]:\n",
    "        return 1\n",
    "    elif data in [\"high\", \"good\", \"big\"]:\n",
    "        return 2\n",
    "    elif data in [\"vhigh\",\"vgood\"]:\n",
    "        return 3\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming data to numbers\n",
    "df_q = df.copy()\n",
    "df_q['maint'] = df_q['maint'].apply(quantify)\n",
    "df_q['doors'] = df_q['doors'].apply(lambda x: int(5) if x == \"5more\" else int(x))\n",
    "df_q['boot'] = df_q['boot'].apply(quantify)\n",
    "df_q['safety'] = df_q['safety'].apply(quantify)\n",
    "df_q['class'] = df_q['class'].apply(quantify)\n",
    "df_q['buying'] = df_q['buying'].apply(quantify)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1728 entries, 0 to 1727\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   maint   1728 non-null   int64\n",
      " 1   doors   1728 non-null   int64\n",
      " 2   boot    1728 non-null   int64\n",
      " 3   safety  1728 non-null   int64\n",
      " 4   class   1728 non-null   int64\n",
      " 5   buying  1728 non-null   int64\n",
      "dtypes: int64(6)\n",
      "memory usage: 81.1 KB\n"
     ]
    }
   ],
   "source": [
    "df_q.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1555, 5) (173, 5) (1555,) (173,)\n"
     ]
    }
   ],
   "source": [
    "# Create X and y variables\n",
    "X = df_q.drop(columns = ['buying'])\n",
    "y = df_q['buying']\n",
    "\n",
    "# Split the dataset by 0.75 and 0.25\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, stratify = y, train_size=0.9)\n",
    "\n",
    "# Check the shape of both train and val datasets\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Random forest is a supervised learning algorithm that can be used for classification. As the name suggests, this algorithm creates multiple decision trees on randomly selected samples and get prediction from each tree. Each tree will have equal votes and tree with the most votes is chosen as the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe = Pipeline([\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ])\n",
    "# Set up hyperparameters tuning\n",
    "rf_params = {\n",
    "    'rf__n_estimators':range(50, 200, 50),\n",
    "    'rf__max_depth':range(30, 40),\n",
    "    'rf__min_samples_leaf':range(1, 40, 10),\n",
    "    'rf__random_state':[123]                       # [123]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Pipeline(steps=[('rf', RandomForestClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'rf__max_depth': range(30, 40),\n",
       "                         'rf__min_samples_leaf': range(1, 40, 10),\n",
       "                         'rf__n_estimators': range(50, 200, 50),\n",
       "                         'rf__random_state': [123]},\n",
       "             scoring='f1_micro', verbose=5)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(rf_pipe, param_grid = rf_params, cv = 5, scoring = 'f1_micro', verbose = 5, n_jobs = -1)\n",
    "\n",
    "# Fit the model\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2784565916398714\n"
     ]
    }
   ],
   "source": [
    "# Get the best parameters\n",
    "best_params = gs.best_params_\n",
    "\n",
    "# Get the best model\n",
    "best_model = gs.best_estimator_\n",
    "\n",
    "# # Predict validation set\n",
    "pred = best_model.predict(X_val)\n",
    "\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost\n",
    "Gradient Boost is another boosting ensemble model that takes an iterative approach to combining weak learners to create a strong learner by focusing on mistakes of prior iterations. For Gradient Boost, all the models are weighed equally and their predictive capacity is restricted with learning rate to increase accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_pipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "        ('gb', GradientBoostingClassifier())\n",
    "    ])\n",
    "# Set up hyperparameters tuning\n",
    "gb_params = {\n",
    "    'gb__learning_rate':  [0.1, 0.25, 0.5],            \n",
    "    'gb__n_estimators': [100, 150, 200, 300],\n",
    "    'gb__max_depth': [3, 5, 7],\n",
    "    'gb__random_state': [123],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scale', StandardScaler()),\n",
       "                                       ('gb', GradientBoostingClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'gb__learning_rate': [0.1, 0.25, 0.5],\n",
       "                         'gb__max_depth': [3, 5, 7],\n",
       "                         'gb__n_estimators': [100, 150, 200, 300],\n",
       "                         'gb__random_state': [123]},\n",
       "             scoring='f1_micro', verbose=1)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GridSearchCV(gb_pipe, param_grid = gb_params, cv = 5, scoring = 'f1_micro', verbose = 1, n_jobs = -1)\n",
    "\n",
    "# Fit the model\n",
    "gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2347266881028939\n",
      "{'gb__learning_rate': 0.1, 'gb__max_depth': 3, 'gb__n_estimators': 100, 'gb__random_state': 123}\n"
     ]
    }
   ],
   "source": [
    "# Get the best parameters\n",
    "best_params = gb.best_params_\n",
    "\n",
    "# Get the best model\n",
    "best_model = gb.best_estimator_\n",
    "\n",
    "# # Predict validation set\n",
    "pred = best_model.predict(X_val)\n",
    "\n",
    "print(gb.best_score_)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "XGBoost, also known as eXtreme Gradient Boosting, is an implementation of gradient boosted decision trees designed for speed and performance. Similar to Gradient Boosting, it is an ensemble tree method that applies the principle of boosting weak learners using the gradient descent architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "        ('xgb', XGBClassifier(objective = 'multi:softmax',\n",
    "                              scale_pos_weight = 1,\n",
    "                              seed = 123,\n",
    "                              booster = 'gbtree',\n",
    "                              eval_metric = None, \n",
    "                              use_label_encoder = False,\n",
    "                              n_jobs = -1))\n",
    "    ])\n",
    "xgb_params = {\n",
    "    'xgb__n_estimators': [200,225,250,300],\n",
    "    'xgb__max_depth': [1,3,5,7],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[01:40:33] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[01:40:33] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scale', StandardScaler()),\n",
       "                                       ('xgb',\n",
       "                                        XGBClassifier(base_score=None,\n",
       "                                                      booster='gbtree',\n",
       "                                                      colsample_bylevel=None,\n",
       "                                                      colsample_bynode=None,\n",
       "                                                      colsample_bytree=None,\n",
       "                                                      enable_categorical=False,\n",
       "                                                      eval_metric=None,\n",
       "                                                      gamma=None, gpu_id=None,\n",
       "                                                      importance_type=None,\n",
       "                                                      interaction_constraints=None,\n",
       "                                                      learning_rate=None,\n",
       "                                                      max_delta_step=None,\n",
       "                                                      max_...\n",
       "                                                      monotone_constraints=None,\n",
       "                                                      n_estimators=100,\n",
       "                                                      n_jobs=-1,\n",
       "                                                      num_parallel_tree=None,\n",
       "                                                      objective='multi:softmax',\n",
       "                                                      predictor=None,\n",
       "                                                      random_state=None,\n",
       "                                                      reg_alpha=None,\n",
       "                                                      reg_lambda=None,\n",
       "                                                      scale_pos_weight=1,\n",
       "                                                      seed=123, subsample=None,\n",
       "                                                      tree_method=None,\n",
       "                                                      use_label_encoder=False, ...))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'xgb__max_depth': [1, 3, 5, 7],\n",
       "                         'xgb__n_estimators': [200, 225, 250, 300]},\n",
       "             scoring='f1_micro', verbose=1)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = GridSearchCV(xgb_pipe, param_grid = xgb_params, cv = 5, scoring = 'f1_micro', verbose = 1, n_jobs = -1)\n",
    "\n",
    "# Fit the model\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33054662379421224\n",
      "{'xgb__max_depth': 1, 'xgb__n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "# Get the best parameters\n",
    "best_params = xgb.best_params_\n",
    "\n",
    "# Get the best model\n",
    "best_model = xgb.best_estimator_\n",
    "\n",
    "# # Predict validation set\n",
    "pred = best_model.predict(X_val)\n",
    "\n",
    "print(xgb.best_score_)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   maint  doors  boot  safety  class\n",
       "0      2      4     2       2      2"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('testing_data/test.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = best_model.predict(test_df)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
