{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split,RepeatedStratifiedKFold,cross_val_score,GridSearchCV\n",
    "from numpy import mean, std\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from xgboost import XGBClassifier,XGBRegressor,XGBRFClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem analysis: This is a multiclass classification machine learning exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data into pandas DataFrame\n",
    "df = pd.read_csv('training_data/car.data', names=['buying', 'maint', 'doors', 'persons', 'boot', 'safety', 'class'])\n",
    "#Leave out persons column|\n",
    "df=df[['maint','doors','boot','safety','class','buying']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vhigh    0.25\n",
       "high     0.25\n",
       "med      0.25\n",
       "low      0.25\n",
       "Name: buying, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the distribution of target column\n",
    "df['buying'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target column distribution is quite balanced\n",
    "\n",
    "#### Encoding data (quantifying good, very good etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['vhigh', 'high', 'med', 'low'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking unique values\n",
    "df['maint'].unique()\n",
    "df['doors'].unique()\n",
    "df['safety'].unique()\n",
    "df['class'].unique()\n",
    "df['buying'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantify(data):\n",
    "    if data in [\"low\",\"unacc\",\"small\"]:\n",
    "        return 0\n",
    "    elif data in [\"med\",\"acc\"]:\n",
    "        return 1\n",
    "    elif data in [\"high\", \"good\", \"big\"]:\n",
    "        return 2\n",
    "    elif data in [\"vhigh\",\"vgood\"]:\n",
    "        return 3\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding data\n",
    "df_q = df.copy()\n",
    "df_q['maint'] = df_q['maint'].apply(quantify)\n",
    "df_q['doors'] = df_q['doors'].apply(lambda x: int(5) if x == \"5more\" else int(x))\n",
    "df_q['boot'] = df_q['boot'].apply(quantify)\n",
    "df_q['safety'] = df_q['safety'].apply(quantify)\n",
    "df_q['class'] = df_q['class'].apply(quantify)\n",
    "df_q['buying'] = df_q['buying'].apply(quantify)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1728 entries, 0 to 1727\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   maint   1728 non-null   int64\n",
      " 1   doors   1728 non-null   int64\n",
      " 2   boot    1728 non-null   int64\n",
      " 3   safety  1728 non-null   int64\n",
      " 4   class   1728 non-null   int64\n",
      " 5   buying  1728 non-null   int64\n",
      "dtypes: int64(6)\n",
      "memory usage: 81.1 KB\n"
     ]
    }
   ],
   "source": [
    "df_q.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1555, 5) (173, 5) (1555,) (173,)\n"
     ]
    }
   ],
   "source": [
    "# Create X and y variables\n",
    "X = df_q.drop(columns = ['buying'])\n",
    "y = df_q['buying']\n",
    "\n",
    "# Split the dataset by 0.9 and 0.1\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, stratify = y, train_size=0.9)\n",
    "\n",
    "# Check the shape of both train and val datasets\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Random forest is a supervised learning algorithm that can be used for classification. As the name suggests, this algorithm creates multiple decision trees on randomly selected samples and get prediction from each tree. Each tree will have equal votes and tree with the most votes is chosen as the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe = Pipeline([\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ])\n",
    "# Set up hyperparameters tuning\n",
    "rf_params = {\n",
    "    'rf__n_estimators':range(20, 50),\n",
    "    'rf__max_depth':range(20, 40),\n",
    "    'rf__min_samples_leaf':range(1, 40, 10),\n",
    "    'rf__random_state':[123]                       # [123]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2400 candidates, totalling 12000 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Pipeline(steps=[('rf', RandomForestClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'rf__max_depth': range(20, 40),\n",
       "                         'rf__min_samples_leaf': range(1, 40, 10),\n",
       "                         'rf__n_estimators': range(20, 50),\n",
       "                         'rf__random_state': [123]},\n",
       "             scoring='f1_micro', verbose=5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(rf_pipe, param_grid = rf_params, cv = 5, scoring = 'f1_micro', verbose = 5, n_jobs = -1)\n",
    "\n",
    "# Fit the model\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2906752411575563\n",
      "Pipeline(steps=[('rf',\n",
      "                 RandomForestClassifier(max_depth=20, min_samples_leaf=31,\n",
      "                                        n_estimators=22, random_state=123))])\n"
     ]
    }
   ],
   "source": [
    "# Get the best parameters\n",
    "best_params = gs.best_params_\n",
    "\n",
    "# Get the best model\n",
    "best_model = gs.best_estimator_\n",
    "\n",
    "#Check the best score from the best model\n",
    "print(gs.best_score_)\n",
    "print(gs.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: Best score is low, we will use random forest as a benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost\n",
    "Gradient Boost is another boosting ensemble model that takes an iterative approach to combining weak learners to create a strong learner by focusing on mistakes of prior iterations. For Gradient Boost, all the models are weighed equally and their predictive capacity is restricted with learning rate to increase accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_pipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "        ('gb', GradientBoostingClassifier())\n",
    "    ])\n",
    "# Set up hyperparameters tuning\n",
    "gb_params = {\n",
    "    'gb__learning_rate':  [0.05, 0.1, 0.25, 0.5],            \n",
    "    'gb__n_estimators': [50, 100, 200, 300],\n",
    "    'gb__max_depth': [1,2,3,4,5],\n",
    "    'gb__random_state': [123],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scale', StandardScaler()),\n",
       "                                       ('gb', GradientBoostingClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'gb__learning_rate': [0.05, 0.1, 0.25, 0.5],\n",
       "                         'gb__max_depth': [1, 2, 3, 4, 5],\n",
       "                         'gb__n_estimators': [50, 100, 200, 300],\n",
       "                         'gb__random_state': [123]},\n",
       "             scoring='f1_micro', verbose=1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GridSearchCV(gb_pipe, param_grid = gb_params, cv = 5, scoring = 'f1_micro', verbose = 1, n_jobs = -1)\n",
    "\n",
    "# Fit the model\n",
    "gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3183279742765273\n",
      "{'gb__learning_rate': 0.05, 'gb__max_depth': 1, 'gb__n_estimators': 50, 'gb__random_state': 123}\n"
     ]
    }
   ],
   "source": [
    "# Get the best parameters\n",
    "best_params = gb.best_params_\n",
    "\n",
    "# Get the best model\n",
    "best_model = gb.best_estimator_\n",
    "\n",
    "#Check the best score from the best model\n",
    "print(gb.best_score_)\n",
    "print(gb.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boost provides an improvement in the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "XGBoost, also known as eXtreme Gradient Boosting, is an implementation of gradient boosted decision trees designed for speed and performance. Similar to Gradient Boosting, it is an ensemble tree method that applies the principle of boosting weak learners using the gradient descent architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "        ('xgb', XGBClassifier(objective = 'multi:softmax',\n",
    "                              scale_pos_weight = 1,\n",
    "                              seed = 123,\n",
    "                              booster = 'gbtree',\n",
    "                              eval_metric = None, \n",
    "                              use_label_encoder = False,\n",
    "                              n_jobs = -1))\n",
    "    ])\n",
    "xgb_params = {\n",
    "    'xgb__n_estimators': [200,225,250,300],\n",
    "    'xgb__max_depth': [1,3,5,7],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[10:44:18] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:44:18] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scale', StandardScaler()),\n",
       "                                       ('xgb',\n",
       "                                        XGBClassifier(base_score=None,\n",
       "                                                      booster='gbtree',\n",
       "                                                      colsample_bylevel=None,\n",
       "                                                      colsample_bynode=None,\n",
       "                                                      colsample_bytree=None,\n",
       "                                                      enable_categorical=False,\n",
       "                                                      eval_metric=None,\n",
       "                                                      gamma=None, gpu_id=None,\n",
       "                                                      importance_type=None,\n",
       "                                                      interaction_constraints=None,\n",
       "                                                      learning_rate=None,\n",
       "                                                      max_delta_step=None,\n",
       "                                                      max_...\n",
       "                                                      monotone_constraints=None,\n",
       "                                                      n_estimators=100,\n",
       "                                                      n_jobs=-1,\n",
       "                                                      num_parallel_tree=None,\n",
       "                                                      objective='multi:softmax',\n",
       "                                                      predictor=None,\n",
       "                                                      random_state=None,\n",
       "                                                      reg_alpha=None,\n",
       "                                                      reg_lambda=None,\n",
       "                                                      scale_pos_weight=1,\n",
       "                                                      seed=123, subsample=None,\n",
       "                                                      tree_method=None,\n",
       "                                                      use_label_encoder=False, ...))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'xgb__max_depth': [1, 3, 5, 7],\n",
       "                         'xgb__n_estimators': [200, 225, 250, 300]},\n",
       "             scoring='f1_micro', verbose=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = GridSearchCV(xgb_pipe, param_grid = xgb_params, cv = 5, scoring = 'f1_micro', verbose = 1, n_jobs = -1)\n",
    "\n",
    "# Fit the model\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3067524115755627\n",
      "{'xgb__max_depth': 1, 'xgb__n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "# Get the best parameters\n",
    "best_params = xgb.best_params_\n",
    "\n",
    "# Get the best model\n",
    "best_model = xgb.best_estimator_\n",
    "\n",
    "print(xgb.best_score_)\n",
    "print(xgb.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: Since Gradient Boosting provides the overall highest score, we will use it to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   maint  doors  boot  safety  class\n",
       "0      2      4     2       2      2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('testing_data/test.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = gb.best_estimator_.predict(test_df)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: This is an exercise to use Machine learning model for multiclass classification prediction. Three models have been used to train with the dataset with the highest score model chosen to predict the given dataset.\n",
    "### Based on the prediction using Gradient Boost, the buying price of the car is low. This finding is solely based on the model output and with a caveat of the limited time available for the model training\n",
    "### Overall the reliability and accuracy of the model is low, particularly due to the small number of training data sets. Ideally with less time constraint, feature engineering can be done to transform the datasets into features which can improve the predictive model overall accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
